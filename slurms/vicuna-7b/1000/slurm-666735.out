11/16/2023 14:37:43 - INFO - __main__ -   Args are parsed. And as follow: 
 model_name: vicuna-7b
device: cuda
file_path: /mnt/rstor/CSE_CSDS_VXC204/sxz517/mh_model_edit/code/
seed: 100
subquestion_breakdown: False
fact_retrieve: True
cot_contradiction: False
output_dir: /mnt/rstor/CSE_CSDS_VXC204/sxz517/mh_model_edit/output/
delete_duplicate_output_file: True
start: 0
end: 3000
fact_query_on: subquestion
edit_num: 1000
holistic_cot: False
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
11/16/2023 14:38:34 - INFO - __main__ -   Files are read and ready to be used!
  0%|          | 0/36 [00:00<?, ?it/s]  3%|▎         | 1/36 [00:00<00:24,  1.41it/s] 22%|██▏       | 8/36 [00:00<00:02, 12.68it/s] 44%|████▍     | 16/36 [00:00<00:00, 24.89it/s] 67%|██████▋   | 24/36 [00:01<00:00, 36.39it/s] 92%|█████████▏| 33/36 [00:01<00:00, 47.94it/s]100%|██████████| 36/36 [00:01<00:00, 30.54it/s]
11/16/2023 14:38:37 - INFO - __main__ -   Prepare works are Done!
  0%|          | 0/3000 [00:00<?, ?it/s]11/16/2023 14:38:45 - INFO - __main__ -   1, 1
  0%|          | 1/3000 [00:07<6:28:15,  7.77s/it]11/16/2023 14:39:02 - INFO - __main__ -   1, 2
  0%|          | 2/3000 [00:25<11:09:07, 13.39s/it]11/16/2023 14:39:22 - INFO - __main__ -   1, 3
  0%|          | 3/3000 [00:45<13:39:05, 16.40s/it]11/16/2023 14:39:37 - INFO - __main__ -   1, 4
  0%|          | 4/3000 [00:59<13:09:42, 15.82s/it]11/16/2023 14:39:51 - INFO - __main__ -   1, 5
  0%|          | 5/3000 [01:13<12:22:27, 14.87s/it]11/16/2023 14:40:13 - INFO - __main__ -   1, 6
  0%|          | 6/3000 [01:35<14:30:53, 17.45s/it]11/16/2023 14:40:30 - INFO - __main__ -   1, 7
  0%|          | 7/3000 [01:52<14:24:23, 17.33s/it]11/16/2023 14:40:52 - INFO - __main__ -   1, 8
  0%|          | 8/3000 [02:14<15:31:52, 18.69s/it]11/16/2023 14:41:15 - INFO - __main__ -   1, 9
  0%|          | 9/3000 [02:37<16:44:30, 20.15s/it]11/16/2023 14:41:29 - INFO - __main__ -   1, 10
  0%|          | 10/3000 [02:51<15:04:32, 18.15s/it]11/16/2023 14:41:45 - INFO - __main__ -   1, 11
  0%|          | 11/3000 [03:07<14:29:25, 17.45s/it]11/16/2023 14:41:57 - INFO - __main__ -   1, 12
  0%|          | 12/3000 [03:19<13:11:08, 15.89s/it]11/16/2023 14:42:16 - INFO - __main__ -   2, 13
  0%|          | 13/3000 [03:38<14:00:55, 16.89s/it]11/16/2023 14:42:36 - INFO - __main__ -   2, 14
  0%|          | 14/3000 [03:58<14:42:56, 17.74s/it]11/16/2023 14:42:50 - INFO - __main__ -   2, 15
  0%|          | 15/3000 [04:12<13:51:43, 16.72s/it]11/16/2023 14:43:06 - INFO - __main__ -   2, 16
  1%|          | 16/3000 [04:28<13:35:32, 16.40s/it]11/16/2023 14:43:24 - INFO - __main__ -   2, 17
  1%|          | 17/3000 [04:46<13:59:28, 16.89s/it]11/16/2023 14:43:45 - INFO - __main__ -   2, 18
  1%|          | 18/3000 [05:07<15:00:21, 18.12s/it]11/16/2023 14:44:04 - INFO - __main__ -   2, 19
  1%|          | 19/3000 [05:26<15:20:09, 18.52s/it]11/16/2023 14:44:21 - INFO - __main__ -   2, 20
  1%|          | 20/3000 [05:43<14:50:51, 17.94s/it]11/16/2023 14:44:36 - INFO - __main__ -   2, 21
  1%|          | 21/3000 [05:59<14:15:07, 17.22s/it]11/16/2023 14:44:59 - INFO - __main__ -   2, 22
  1%|          | 22/3000 [06:22<15:41:24, 18.97s/it]11/16/2023 14:45:18 - INFO - __main__ -   2, 23
  1%|          | 23/3000 [06:40<15:40:18, 18.95s/it]11/16/2023 14:45:31 - INFO - __main__ -   2, 24
  1%|          | 24/3000 [06:53<14:11:11, 17.16s/it]11/16/2023 14:45:48 - INFO - __main__ -   2, 25
  1%|          | 25/3000 [07:10<14:00:50, 16.96s/it]11/16/2023 14:46:02 - INFO - __main__ -   2, 26
  1%|          | 26/3000 [07:25<13:26:19, 16.27s/it]11/16/2023 14:46:18 - INFO - __main__ -   2, 27
  1%|          | 27/3000 [07:40<13:17:24, 16.09s/it]11/16/2023 14:46:32 - INFO - __main__ -   2, 28
  1%|          | 28/3000 [07:54<12:38:07, 15.31s/it]11/16/2023 14:46:48 - INFO - __main__ -   2, 29
  1%|          | 29/3000 [08:10<12:52:52, 15.61s/it]11/16/2023 14:47:10 - INFO - __main__ -   2, 30
  1%|          | 30/3000 [08:32<14:28:57, 17.55s/it]11/16/2023 14:47:23 - INFO - __main__ -   2, 31
  1%|          | 31/3000 [08:45<13:16:02, 16.09s/it]11/16/2023 14:47:33 - INFO - __main__ -   2, 32
  1%|          | 32/3000 [08:55<11:53:51, 14.43s/it]11/16/2023 14:47:41 - INFO - __main__ -   3, 33
  1%|          | 33/3000 [09:04<10:21:32, 12.57s/it]11/16/2023 14:47:55 - INFO - __main__ -   3, 34
  1%|          | 34/3000 [09:18<10:41:22, 12.97s/it]11/16/2023 14:48:08 - INFO - __main__ -   3, 35
  1%|          | 35/3000 [09:31<10:42:46, 13.01s/it]11/16/2023 14:48:25 - INFO - __main__ -   3, 36
  1%|          | 36/3000 [09:48<11:39:41, 14.16s/it]slurmstepd: error: *** JOB 666735 ON aisct04 CANCELLED AT 2023-11-16T14:48:30 ***
